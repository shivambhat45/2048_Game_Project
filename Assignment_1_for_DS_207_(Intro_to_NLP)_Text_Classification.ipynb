{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivambhat45/2048_Game_Project/blob/master/Assignment_1_for_DS_207_(Intro_to_NLP)_Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PHRmq-UanuC"
      },
      "source": [
        "# Assignment 1: Text Classification, Word Vectors (TA: Tarun Gupta)\n",
        "\n",
        "The goal of this assignment is introduce the basics of text classification and word vectors.\n",
        "\n",
        "Please make a copy of this notebook (locally or on Colab). Ensure you adhere to the guidelines and submission instructions (mentioned below) for attempting and submitting the assignment.\n",
        "\n",
        "Given that the class has 150+ students, we will **NOT** entertain any requests for changing your notebooks after the submission deadline (especially in cases when the notebook fails to compile or run because you did not follow the instructions)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5xWUYtEanuQ"
      },
      "source": [
        "### Guidelines for Attempting the Assignment\n",
        "\n",
        "1. Write your logic in the cells **ONLY** which have the comment `# ADD YOUR CODE HERE`, between the `# BEGIN CODE` and `# END CODE` comments. These cells are also demarcated by the special start (`## ==== BEGIN EVALUATION PORTION`) and end (`## ==== END EVALUATION PORTION`) comments. Do **NOT** remove any of these comments from the designated cells, otherwise your assignment will not be evaluated correctly.\n",
        "\n",
        "2. Write your code **ONLY** in the cells designated for auto-evaluation, between the `# BEGIN CODE` and `# END CODE` comments. Please don't write any extra code or comments anywhere else.\n",
        "\n",
        "3. All imports that should be necessary are already provided as part of the notebook. You should **NOT** import any new libraries, otherwise your assignment will not be graded.\n",
        "\n",
        "4. You need to install the libraries/imports used in this notebook yourself. Its recommended to use python version between 3.9 and 3.11 to attempt this assignment.\n",
        "\n",
        "5. **If you encounter any errors in the supporting cells during execution, contact the respective TAs.**\n",
        "\n",
        "6. **Please read the function docs and comments carefully**. They provide specific instructions and examples for implementing each function. Follow these instructions precisely - neither oversimplify nor overcomplicate your implementations. Deviating from the provided implementation guidelines may result in lost marks.\n",
        "\n",
        "7. **Important**: Use of AI-assistive technologies such as ChatGPT or GitHub CoPilot is not permitted for this assignment. Ensure that all attempts are solely your own. Not following this rule can incur a large penalty, including but not limited to scoring a zero for this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAnnlHNcanuT"
      },
      "source": [
        "### Submission Instructions\n",
        "\n",
        "1. Ensure your code follows all guidelines mentioned above before submission.\n",
        "\n",
        "2. Ensure you only add code in designated areas, otherwise you assignment will not evaluated.\n",
        "\n",
        "3. When you have completely attempted the assignment, **export the current notebook as a `.py` file**, with the following name: `SAPName_SRNo_assignment1.py`, where `SAPName` would be your name as per SAP record, and `SRNo` will be the last 5 digits of your IISc SR number. For example, IISc student with SAP name Twyla Linda (SR no - 04-03-00-10-22-20-1-15329) would use `Twyla_Linda_15329_assignment1.py`.\n",
        "\n",
        "4. You should put your assignment file `SAPName_SRNo_assignment1.py` inside a folder `SAPName_SRNo`. The folder structure looks as follows:\n",
        "\n",
        "``` python\n",
        "└─── SAPName_SRNo\n",
        "     ├─── SAPName_SRNo_assignment1.py\n",
        "```\n",
        "\n",
        "5. When you run the assignment code, it may download certain datasets and other artifacts. These should **NOT** be part of the above folder.\n",
        "\n",
        "5. Once you have validated the folder structure as above, zip the folder and name it as `submission.zip` and submit this ZIP archive.\n",
        "\n",
        "**If you have any confusion regarding submission instructions, please ask the respective TAs.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzRjdPLyanuW"
      },
      "source": [
        "### Marks Distribution\n",
        "\n",
        "- Generative Classification: 40 marks\n",
        "- Word2Vec and Word Analogies: 30 marks\n",
        "- Discriminative Classification: 30 marks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2M-eUOyanuX"
      },
      "source": [
        "In the cell below, replace `SAPName` with your name as per SAP record, and `SRNo` with the last 5 digits of your IISc SR number. For example, IISc student with SAP name Twyla Linda (SR no - 04-03-00-10-22-20-1-15329) would use:\n",
        "\n",
        "```python\n",
        "STUDENT_SAP_NAME  = \"Twyla Linda\"\n",
        "STUDENT_SR_NUMBER = \"15329\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OIc67JYanuZ"
      },
      "outputs": [],
      "source": [
        "STUDENT_SAP_NAME  = \"SAPName\"\n",
        "STUDENT_SR_NUMBER = \"SRNo\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7V2xJdCanuc"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIu94tsWanud"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from pathlib import Path\n",
        "import nltk\n",
        "from gensim.models import KeyedVectors\n",
        "import gensim.downloader as api\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dzvx8891anue"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "We will dive into a basic text-based sentiment classification task. The dataset consists of sentences with two different kinds of sentiments- `1` (`positive`), and `0` (`negative`) sentiments. Following are a set of examples,\n",
        "\n",
        "* **`1`**: *I really like your new haircut!*\n",
        "* **`0`**: *Your new haircut is awful!*\n",
        "\n",
        "Below we download a training set (`train_data.csv`- provided) and a validation set (`val_data.csv`- provided). During evaluation we will use a blind test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8EgYmP_anuf"
      },
      "outputs": [],
      "source": [
        "def download_dataset(url, output_path):\n",
        "    \"\"\"\n",
        "    Download a CSV file from a given URL and save it to the specified path.\n",
        "    If the file already exists, skip the download.\n",
        "\n",
        "    Parameters:\n",
        "    url (str): URL of the CSV file to download\n",
        "    output_path (str): Path where the file should be saved\n",
        "\n",
        "    Returns:\n",
        "    bool: True if download was successful or file already exists, False otherwise\n",
        "    \"\"\"\n",
        "    # Convert to Path object for easier path manipulation\n",
        "    output_path = Path(output_path)\n",
        "\n",
        "    # Check if file already exists\n",
        "    if output_path.exists():\n",
        "        print(f\"File already exists: {output_path}\")\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        print(f\"Downloading {output_path.name}...\")\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "        # Create parent directories if they don't exist\n",
        "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        with open(output_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Successfully downloaded: {output_path}\")\n",
        "        return True\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading {output_path}: {e}\")\n",
        "        return False\n",
        "    except IOError as e:\n",
        "        print(f\"Error saving file {output_path}: {e}\")\n",
        "        return False\n",
        "\n",
        "# URLs for the datasets\n",
        "urls = {\n",
        "    'train': \"https://docs.google.com/spreadsheets/d/1pHK8joOen4R1KlhF5Re3LZFb4Dt0n4jraHoTGMBgtF4/export?format=csv\",\n",
        "    'val': \"https://docs.google.com/spreadsheets/d/1t2J2EJPo-P2AlDOAybxq7nX61mbZwgMoQpR_J3FneJ4/export?format=csv\",\n",
        "}\n",
        "\n",
        "# Download all datasets\n",
        "for dataset_type, url in urls.items():\n",
        "    output_path = f\"downloaded_datasets/{dataset_type}_data.csv\"\n",
        "    download_dataset(url, output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Zb86YZGanuh"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('downloaded_datasets/train_data.csv')\n",
        "df_val = pd.read_csv('downloaded_datasets/val_data.csv')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7jX7kchanuh"
      },
      "outputs": [],
      "source": [
        "sentiment_percentages = df_val['sentiment'].value_counts(normalize=True) * 100\n",
        "\n",
        "print(\"Sentiment Distribution:\")\n",
        "print(f\"Class 0: {sentiment_percentages[0]:.2f}%\")\n",
        "print(f\"Class 1: {sentiment_percentages[1]:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4SWh7KEanui"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = df.review.values.tolist(), df.sentiment.values.tolist()\n",
        "X_val, y_val = df_val.review.values.tolist(), df_val.sentiment.values.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYwmL0jAanuj"
      },
      "source": [
        "# General evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpGlFI1Hanuj"
      },
      "outputs": [],
      "source": [
        "def evaluate_classifier(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate classification accuracy.\n",
        "\n",
        "    Args:\n",
        "        y_true (list): True class labels\n",
        "            Example: [0, 1, 0, 1]\n",
        "        y_pred (list): Predicted class labels\n",
        "            Example: [0, 1, 1, 1]\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy (proportion of correct predictions)\n",
        "            Example: 0.75 (3 correct predictions out of 4)\n",
        "\n",
        "    Note:\n",
        "        - Accuracy = (number of correct predictions) / (total number of predictions)\n",
        "        - Raises ValueError if lengths of inputs don't match\n",
        "    \"\"\"\n",
        "    if len(y_true) != len(y_pred):\n",
        "        raise ValueError(\"Length of true and predicted labels must match\")\n",
        "\n",
        "    correct = sum(1 for t, p in zip(y_true, y_pred) if t == p)\n",
        "    return correct / len(y_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAR6vMplanuk"
      },
      "source": [
        "# Generative Classification\n",
        "\n",
        "## Naive Bayes Text Classification\n",
        "\n",
        " This implementation covers a Naive Bayes classifier for text classification.\n",
        " The key mathematical foundation is Bayes' theorem:\n",
        "\n",
        " P(class|document) ∝ P(class) * P(document|class)\n",
        "\n",
        " Where:\n",
        " - P(class|document) is the posterior probability\n",
        " - P(class) is the prior probability of the class\n",
        " - P(document|class) is the likelihood of the document given the class\n",
        "\n",
        " Under the \"naive\" assumption of conditional independence:\n",
        " P(document|class) = P(word1|class) * P(word2|class) * ... * P(wordN|class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVaWWK5Tanul"
      },
      "outputs": [],
      "source": [
        "# ==== BEGIN EVALUATION PORTION\n",
        "\n",
        "class NaiveBayesClassifier:\n",
        "    def __init__(self, min_freq=1):\n",
        "        \"\"\"\n",
        "        Initialize the Naive Bayes classifier.\n",
        "\n",
        "        Args:\n",
        "            min_freq (int): Minimum frequency threshold for a word to be included in vocabulary.\n",
        "                           Words appearing less than min_freq times will be treated as UNK token.\n",
        "                           Default: 1 (include all words)\n",
        "\n",
        "        Attributes:\n",
        "            class_probs (dict): P(class) for each class\n",
        "                Example: {0: 0.5, 1: 0.5}\n",
        "\n",
        "            word_probs (dict): P(word|class) for each word and class\n",
        "                Example: {\n",
        "                    0: {'hello': 0.5, 'world': 0.4, '<UNK>': 0.1},\n",
        "                    1: {'hello': 0.3, 'world': 0.5, '<UNK>': 0.2}\n",
        "                }\n",
        "\n",
        "            vocabulary (dict): Word to index mapping, including special UNK token\n",
        "                Example: {'<UNK>': 0, 'hello': 1, 'world': 2}\n",
        "\n",
        "            min_freq (int): Minimum frequency threshold for vocabulary inclusion\n",
        "                Example: If min_freq=2, words must appear at least twice to be included\n",
        "\n",
        "        Note:\n",
        "            - Words appearing less than min_freq times in training data will be mapped to <UNK>\n",
        "            - <UNK> token is automatically added to vocabulary as first token (index 0)\n",
        "            - Probability for <UNK> is calculated during training based on rare words\n",
        "        \"\"\"\n",
        "        self.class_probs = None\n",
        "        self.word_probs = None\n",
        "        self.vocabulary = None\n",
        "        self.min_freq = min_freq\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Preprocess the input text by converting to lowercase, removing non-word characters,\n",
        "        and filtering out common stop words.\n",
        "\n",
        "        Args:\n",
        "            text (str): Raw input text\n",
        "                Example: \"Hello, World! How are you doing today?\"\n",
        "\n",
        "        Returns:\n",
        "            list: List of cleaned, tokenized, and filtered words with stop words removed\n",
        "                Example: ['hello', 'world', 'doing', 'today']\n",
        "\n",
        "        Note:\n",
        "            - Converts all text to lowercase\n",
        "            - Removes punctuation and special characters\n",
        "            - Splits text into individual tokens\n",
        "            - Removes common English stop words (e.g., 'a', 'an', 'the', 'is', 'are', 'how')\n",
        "            - Stop words are removed using NLTK's English stop words list\n",
        "        \"\"\"\n",
        "        # Import stop words from NLTK\n",
        "        from nltk.corpus import stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Extract word characters only and split into tokens\n",
        "        tokens = re.findall(r'\\w+', text)\n",
        "\n",
        "        # Remove stop words\n",
        "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "        return filtered_tokens\n",
        "\n",
        "    def create_vocabulary(self, texts):\n",
        "        \"\"\"\n",
        "        Create vocabulary from training texts by mapping unique words to indices,\n",
        "        considering minimum frequency threshold and adding UNK token.\n",
        "\n",
        "        Args:\n",
        "            texts (list): List of text documents\n",
        "                Example: [\n",
        "                    \"Hello world hello\",\n",
        "                    \"Hello there\",\n",
        "                    \"World is beautiful\"\n",
        "                ]\n",
        "\n",
        "        Returns:\n",
        "            dict: Mapping of words to unique indices, including UNK token\n",
        "                Example (with min_freq=2): {\n",
        "                    '<UNK>': 0,    # Special token for rare/unseen words\n",
        "                    'hello': 1,    # Frequency=3, included in vocab\n",
        "                    'world': 2,    # Frequency=2, included in vocab\n",
        "                    # 'there' and 'beautiful' not included (frequency=1 < min_freq=2)\n",
        "                }\n",
        "\n",
        "        Note:\n",
        "            - Always includes <UNK> token at index 0\n",
        "            - Only includes words that appear >= min_freq times\n",
        "            - Word frequency is counted across all documents\n",
        "            - Uses preprocess_text function for preprocessing\n",
        "            - Words below frequency threshold will be mapped to UNK during feature extraction\n",
        "        \"\"\"\n",
        "\n",
        "        # BEGIN CODE : naive_bayes.create_vocabulary\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def extract_features(self, texts, vocabulary):\n",
        "        \"\"\"\n",
        "        Convert texts to bag-of-words feature vectors using the vocabulary,\n",
        "        where each element represents the count of word occurrences (not binary presence/absence).\n",
        "\n",
        "        Args:\n",
        "            texts (list): List of text documents\n",
        "                Example: [\"hello world hello\", \"world is beautiful\"]\n",
        "            vocabulary (dict): Word to index mapping with UNK token\n",
        "                Example: {'<UNK>': 0, 'hello': 1, 'world': 2}\n",
        "\n",
        "        Returns:\n",
        "            np.array: Feature matrix where each row is a document vector\n",
        "                Example: For the above input with min_freq=2:\n",
        "                array([\n",
        "                    [0, 2, 1],  # First doc: 0 UNKs, 2 'hello's, 1 'world'\n",
        "                    [2, 0, 1]   # Second doc: 2 UNKs (one each for 'is' and 'beautiful'), 0 'hello's, 1 'world',\n",
        "                ])\n",
        "\n",
        "        Note:\n",
        "            - Each row represents one document\n",
        "            - Each column represents the count of a specific word\n",
        "            - First column is always UNK token count\n",
        "            - Words not in vocabulary are counted as UNK\n",
        "            - Shape of output: (n_documents, len(vocabulary))\n",
        "            - Uses preprocess_text function for preprocessing\n",
        "        \"\"\"\n",
        "\n",
        "        # BEGIN CODE : naive_bayes.extract_features\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def calculate_class_probabilities(self, y):\n",
        "        \"\"\"\n",
        "        Estimate probability P(class) for each class from training labels.\n",
        "\n",
        "        Args:\n",
        "            y (list): List of class labels\n",
        "                Example: [0, 0, 1, 1, 0, 1]\n",
        "\n",
        "        Returns:\n",
        "            dict: Estimated probability for each class\n",
        "                Example: {\n",
        "                    0: 0.5,    # 3 out of 6 samples are class 0\n",
        "                    1: 0.5     # 3 out of 6 samples are class 1\n",
        "                }\n",
        "\n",
        "        Note:\n",
        "            - Probabilities sum to 1 across all classes\n",
        "            - Handles any number of unique classes\n",
        "        \"\"\"\n",
        "        # BEGIN CODE : naive_bayes.extract_features\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def calculate_word_probabilities(self, X, y, vocabulary, alpha=1.0):\n",
        "        \"\"\"\n",
        "        Calculate conditional probability P(word|class) for each word and class,\n",
        "        including probability for UNK token.\n",
        "\n",
        "        Args:\n",
        "            X (np.array): Document-term matrix (with UNK counts in first column)\n",
        "                Example: array([\n",
        "                    [0, 2, 1],  # Document 1: 0 UNKs, 2 of word 1, 1 of word 2\n",
        "                    [1, 0, 1],  # Document 2: 1 UNK, 0 of word 1, 1 of word 2\n",
        "                ])\n",
        "            y (list): Class labels\n",
        "                Example: [0, 1]\n",
        "            vocabulary (dict): Word to index mapping with UNK token\n",
        "                Example: {'<UNK>': 0, 'hello': 1, 'world': 2}\n",
        "            alpha (float): Laplace smoothing parameter, default=1.0\n",
        "\n",
        "        Returns:\n",
        "            dict: Nested dict with P(word|class) for each word and class\n",
        "                Example: {\n",
        "                    0: {\n",
        "                        '<UNK>': 0.167,    # P(word=UNK|class=0)\n",
        "                        'hello': 0.5,     # P(word='hello'|class=0)\n",
        "                        'world': 0.333      # P(word='world'|class=0)\n",
        "                    },\n",
        "                    1: {\n",
        "                        '<UNK>': 0.4,    # P(word=UNK|class=1)\n",
        "                        'hello': 0.2,     # P(word='hello'|class=1)\n",
        "                        'world': 0.4      # P(word='world'|class=1)\n",
        "                    }\n",
        "                }\n",
        "\n",
        "        Note:\n",
        "            - Uses Laplace smoothing to handle unseen words\n",
        "            - UNK token probability is learned from training data\n",
        "            - Formula: P(word|class) = (count(word,class) + α) / (total_words_in_class + α|V|)\n",
        "            - |V| is vocabulary size (including UNK token)\n",
        "        \"\"\"\n",
        "\n",
        "        # BEGIN CODE : naive_bayes.calculate_word_probabilities\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def fit(self, X_text, y):\n",
        "        \"\"\"\n",
        "        Train the Naive Bayes classifier on the provided text documents.\n",
        "\n",
        "        Args:\n",
        "            X_text (list): List of text documents\n",
        "                Example: [\n",
        "                    \"hello world\",\n",
        "                    \"beautiful world\",\n",
        "                    \"hello there\"\n",
        "                ]\n",
        "            y (list): Class labels\n",
        "                Example: [0, 1, 0]\n",
        "\n",
        "        Note:\n",
        "            - Creates vocabulary from training texts\n",
        "            - Calculates prior probabilities P(class)\n",
        "            - Calculates conditional probabilities P(word|class)\n",
        "            - Stores all necessary parameters for prediction\n",
        "        \"\"\"\n",
        "        # Create vocabulary from training texts\n",
        "        self.vocabulary = self.create_vocabulary(X_text)\n",
        "\n",
        "        # Convert texts to feature vectors\n",
        "        X = self.extract_features(X_text, self.vocabulary)\n",
        "\n",
        "        # Calculate probabilities\n",
        "        self.class_probs = self.calculate_class_probabilities(y)\n",
        "        self.word_probs = self.calculate_word_probabilities(\n",
        "            X, y, self.vocabulary)\n",
        "\n",
        "    def predict(self, X_text):\n",
        "        \"\"\"\n",
        "        Predict classes for new documents using Naive Bayes algorithm,\n",
        "        handling unknown words using UNK token.\n",
        "\n",
        "        Args:\n",
        "            X_text (list): List of text documents\n",
        "                Example: [\n",
        "                    \"hello world\",\n",
        "                    \"beautiful day\"  # 'day' is unknown, treated as UNK\n",
        "                ]\n",
        "\n",
        "        Returns:\n",
        "            list: Predicted class labels\n",
        "                Example: [0, 1]\n",
        "\n",
        "        Theory:\n",
        "            The standard Naive Bayes formula for text classification is:\n",
        "            P(class|document) ∝ P(class) * ∏ P(word|class)\n",
        "\n",
        "            For unknown words not in vocabulary:\n",
        "            - They are mapped to UNK token\n",
        "            - P(UNK|class) is used in probability calculation\n",
        "\n",
        "            We use log space to prevent numerical underflow:\n",
        "            log(P(class|document)) ∝ log(P(class)) + Σ log(P(word|class))\n",
        "\n",
        "        Implementation:\n",
        "            For each document:\n",
        "            1. Preprocess and tokenize text\n",
        "            2. Replace unknown words with UNK token\n",
        "            3. Calculate log probabilities using appropriate word or UNK probabilities\n",
        "            4. Return class with highest log probability score\n",
        "\n",
        "        Note:\n",
        "            - Uses preprocess_text function for preprocessing\n",
        "            - Words not in vocabulary are treated as UNK token\n",
        "            - UNK probability is used for out-of-vocabulary words\n",
        "        \"\"\"\n",
        "        # BEGIN CODE : naive_bayes.predict\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def get_important_words(self, n=5, use_ratio=True):\n",
        "        \"\"\"\n",
        "        Get the most important words for each class based either on their raw conditional\n",
        "        probabilities or their probability ratios between classes.\n",
        "\n",
        "        Args:\n",
        "            n (int): Number of top words to return for each class, default=5\n",
        "            use_ratio (bool): If True, ranks words by probability ratio between classes\n",
        "                            If False, ranks words by raw conditional probability\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary mapping class labels to lists of (word, score) tuples,\n",
        "                  where score is either probability or probability ratio\n",
        "                Example with use_ratio=False: {\n",
        "                    0: [('excellent', 0.014), ('great', 0.012), ('amazing', 0.011),\n",
        "                        ('<UNK>', 0.008), ('wonderful', 0.007)],  # Raw probabilities\n",
        "                    1: [('terrible', 0.015), ('bad', 0.012), ('<UNK>', 0.010),\n",
        "                        ('boring', 0.008), ('awful', 0.007)]\n",
        "                }\n",
        "                Example with use_ratio=True: {\n",
        "                    0: [('excellent', 7.5), ('amazing', 6.2), ('great', 5.8),\n",
        "                        ('wonderful', 4.9), ('good', 4.1)],  # P(word|pos)/P(word|neg)\n",
        "                    1: [('terrible', 8.3), ('awful', 7.1), ('bad', 6.4),\n",
        "                        ('boring', 5.2), ('waste', 4.8)]    # P(word|neg)/P(word|pos)\n",
        "                }\n",
        "\n",
        "        Note:\n",
        "            - When use_ratio=True:\n",
        "                - For class 0: Returns words where P(word|class=0)/P(word|class=1) is highest\n",
        "                - For class 1: Returns words where P(word|class=1)/P(word|class=0) is highest\n",
        "                - Better at finding discriminative words that distinguish between classes\n",
        "                - Reduces overlap between top words of different classes\n",
        "            - When use_ratio=False:\n",
        "                - Returns words with highest raw P(word|class) for each class\n",
        "                - May have significant overlap between classes\n",
        "            - Includes UNK token only if it meets the ranking criteria\n",
        "            - Small probabilities are handled safely to avoid division by zero\n",
        "        \"\"\"\n",
        "        if not self.word_probs:\n",
        "            raise ValueError(\"Classifier must be trained before getting important words\")\n",
        "\n",
        "        important_words = {}\n",
        "        classes = sorted(self.word_probs.keys())  # Get classes in consistent order\n",
        "\n",
        "        for cls in classes:\n",
        "            other_cls = [c for c in classes if c != cls][0]  # Get the other class\n",
        "\n",
        "            if use_ratio:\n",
        "                # Calculate probability ratios for all words\n",
        "                word_scores = []\n",
        "                for word in self.vocabulary:\n",
        "                    # Add small epsilon to denominator to avoid division by zero\n",
        "                    ratio = (self.word_probs[cls][word] /\n",
        "                            (self.word_probs[other_cls][word] + 1e-4))\n",
        "                    word_scores.append((word, ratio))\n",
        "            else:\n",
        "                # Use raw probabilities\n",
        "                word_scores = list(self.word_probs[cls].items())\n",
        "\n",
        "            # Sort by score (either ratio or probability) and take top n\n",
        "            sorted_words = sorted(word_scores, key=lambda x: x[1], reverse=True)\n",
        "            important_words[cls] = sorted_words[:n]\n",
        "\n",
        "        return important_words\n",
        "\n",
        "# ==== END EVALUATION PORTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjulkO45anur"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_naive_bayes_example():\n",
        "    \"\"\"\n",
        "    Example demonstrating how to use the NaiveBayesClassifier.\n",
        "    \"\"\"\n",
        "    # Sample training data\n",
        "    X_example_train = [\n",
        "        \"I love this movie\",\n",
        "        \"Great film, amazing actors\",\n",
        "        \"Terrible waste of time\",\n",
        "        \"Poor acting, bad script\",\n",
        "        \"Excellent movie, highly recommend\"\n",
        "    ]\n",
        "    y_example_train = [1, 1, 0, 0, 1]  # 1: positive, 0: negative\n",
        "\n",
        "    # Sample validation data\n",
        "    X_example_val = [\n",
        "        \"Really enjoyed this film\",\n",
        "        \"Waste of money, terrible\"\n",
        "    ]\n",
        "    y_example_val = [1, 0]\n",
        "\n",
        "    # Train classifier\n",
        "    nb_classifier = NaiveBayesClassifier(min_freq=1)\n",
        "    nb_classifier.fit(X_example_train, y_example_train)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = nb_classifier.predict(X_example_val)\n",
        "\n",
        "    # Evaluate\n",
        "    accuracy = evaluate_classifier(y_example_val, predictions)\n",
        "    print(f\"Validation accuracy on this example dataset: {accuracy:.4f}\")\n",
        "\n",
        "    # Get and print important words\n",
        "    important_words = nb_classifier.get_important_words(n=5)\n",
        "    for class_label, words in important_words.items():\n",
        "        sentiment = \"Negative\" if class_label == 0 else \"Positive\"\n",
        "        print(f\"\\nTop words for {sentiment} sentiment:\")\n",
        "        for word, prob in words:\n",
        "            print(f\"{word}: {prob:.4f}\")\n",
        "\n",
        "train_and_evaluate_naive_bayes_example()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMFw5j5Ganuu"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_naive_bayes_main():\n",
        "    \"\"\"\n",
        "    Train and evaluate the Naive Bayes classifier.\n",
        "    \"\"\"\n",
        "    nb_classifier = NaiveBayesClassifier(min_freq=3)\n",
        "    nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "    predictions = nb_classifier.predict(X_val)\n",
        "    accuracy = evaluate_classifier(y_val, predictions)\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Get and print important words\n",
        "    important_words = nb_classifier.get_important_words(n=10)\n",
        "    for class_label, words in important_words.items():\n",
        "        sentiment = \"Negative\" if class_label == 0 else \"Positive\"\n",
        "        print(f\"\\nTop words for {sentiment} sentiment:\")\n",
        "        for word, prob in words:\n",
        "            print(f\"{word}: {prob:.4f}\")\n",
        "\n",
        "# Above 80% validation accuracy is good!\n",
        "train_and_evaluate_naive_bayes_main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXX1CUYaanuv"
      },
      "source": [
        "# Word2Vec and Word Analogies: Understanding Semantic Relationships\n",
        "  \n",
        " Word embeddings have revolutionized NLP by capturing semantic relationships between words in dense vector spaces. Word2Vec, introduced by Mikolov et al. (2013), maps words to continuous vector representations where similar words cluster together and relationships between words are preserved as vector operations.\n",
        "\n",
        "## Key concepts\n",
        " - Words are represented as dense vectors in high-dimensional space (typically 300D)\n",
        " - Similar words have similar vector representations\n",
        " - Vector arithmetic captures semantic relationships\n",
        " - Famous example: king - man + woman ≈ queen\n",
        "\n",
        " This notebook explores implementation and evaluation of word analogies using different similarity metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo7G6XHganuw"
      },
      "source": [
        "## Download word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_qbfgCwanuw"
      },
      "outputs": [],
      "source": [
        "def download_word2vec_model(model_name=\"word2vec-google-news-300\"):\n",
        "    \"\"\"\n",
        "    Download word2vec model using gensim's built-in downloader.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Name of the model to download.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the downloaded model\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the specified model is not available\n",
        "        Exception: For other download or processing errors\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if model is available\n",
        "        available_models = api.info()['models'].keys()\n",
        "        if model_name not in available_models:\n",
        "            raise ValueError(\n",
        "                f\"Model '{model_name}' not found. Available models: {', '.join(available_models)}\"\n",
        "            )\n",
        "\n",
        "        print(f\"Downloading {model_name}...\")\n",
        "        model_path = api.load(model_name, return_path=True)\n",
        "        print(f\"Model downloaded successfully to: {model_path}\")\n",
        "\n",
        "        return model_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading model: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "word2vec_path = download_word2vec_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwAAKlGaanux"
      },
      "source": [
        " ## Vector Operations and Similarity Metrics\n",
        "\n",
        " We implement two key similarity metrics:\n",
        " 1. Cosine Similarity: Measures angle between vectors, normalized to [-1,1]\n",
        " 2. Euclidean Similarity: Based on straight-line distance between vectors\n",
        "\n",
        " The class below handles vector operations and similarity computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeII8hDDanuy"
      },
      "outputs": [],
      "source": [
        "# ==== BEGIN EVALUATION PORTION\n",
        "\n",
        "class WordEmbeddingOps:\n",
        "    def __init__(self, word2vec_path):\n",
        "        \"\"\"\n",
        "        Initialize the WordEmbeddings class with a pre-trained word2vec model.\n",
        "\n",
        "        Args:\n",
        "            word2vec_path (str): Path to the word2vec model file\n",
        "                Example: 'path/to/GoogleNews-vectors-negative300.bin'\n",
        "\n",
        "        Note:\n",
        "            - Loads word vectors using gensim's KeyedVectors\n",
        "        \"\"\"\n",
        "        self.word_vectors = KeyedVectors.load_word2vec_format(\n",
        "            word2vec_path, binary=True)\n",
        "\n",
        "    def cosine_similarity(self, vec1, vec2):\n",
        "        \"\"\"\n",
        "        Calculate cosine similarity between two vectors.\n",
        "\n",
        "        Args:\n",
        "            vec1 (np.array): First vector\n",
        "                Example: array([0.2, 0.5, -0.1])\n",
        "            vec2 (np.array): Second vector\n",
        "                Example: array([0.3, 0.4, -0.2])\n",
        "\n",
        "        Returns:\n",
        "            float: Cosine similarity between vectors\n",
        "                Example: 0.95 (for above vectors)\n",
        "\n",
        "        Note:\n",
        "            - Cosine similarity = vec1 · vec2 / (||vec1|| ||vec2||)\n",
        "            - Range: [-1, 1], where 1 means same direction\n",
        "        \"\"\"\n",
        "        # BEGIN CODE : word_embedding_ops.cosine_similarity\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def euclidean_similarity(self, vec1, vec2):\n",
        "        \"\"\"\n",
        "        Calculate similarity based on Euclidean distance.\n",
        "\n",
        "        Args:\n",
        "            vec1 (np.array): First vector\n",
        "                Example: array([0.2, 0.5, -0.1])\n",
        "            vec2 (np.array): Second vector\n",
        "                Example: array([0.3, 0.4, -0.2])\n",
        "\n",
        "        Returns:\n",
        "            float: Similarity score based on Euclidean distance\n",
        "                Example: 0.85\n",
        "\n",
        "        Note:\n",
        "            - Converts Euclidean distance to similarity\n",
        "            - similarity = 1 / (1 + distance)\n",
        "            - Range: (0, 1], where 1 means identical vectors\n",
        "        \"\"\"\n",
        "        # BEGIN CODE : word_embedding_ops.euclidean_similarity\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def find_analogies(self, word1, word2, word3, similarity_func='cosine', num_results=5):\n",
        "        \"\"\"\n",
        "        Find the words that complete the analogy: word1 : word2 :: word3 : ?\n",
        "\n",
        "        Args:\n",
        "            word1 (str): First word in the analogy\n",
        "                Example: 'king'\n",
        "            word2 (str): Second word in the analogy\n",
        "                Example: 'man'\n",
        "            word3 (str): Third word in the analogy\n",
        "                Example: 'queen'\n",
        "            num_results (int): Number of top results to return\n",
        "                Example: 5\n",
        "            similarity_func (str): Similarity function to use ('cosine' or 'euclidean')\n",
        "\n",
        "        Returns:\n",
        "            list: List of tuples (word, similarity_score) for top num_results matches\n",
        "                Example: [('woman', 0.95), ('girl', 0.82), ('lady', 0.78), ...]\n",
        "\n",
        "        Note:\n",
        "            - Uses vector arithmetic: word2 - word1 + word3\n",
        "            - Excludes input words from results\n",
        "            - Returns empty list if any input word not in vocabulary\n",
        "            - Implementation iterates through all words in vocabulary using:\n",
        "              for word in self.word_vectors.index_to_key\n",
        "              This is necessary to compare the target vector against every\n",
        "              possible word in the model's vocabulary\n",
        "        \"\"\"\n",
        "        # BEGIN CODE : word_embedding_ops.find_analogies\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def find_similar_words(self, word, num_results=5, similarity_func='cosine'):\n",
        "        \"\"\"\n",
        "        Find the most similar words to a given word.\n",
        "\n",
        "        Args:\n",
        "            word (str): Input word to find similar words for\n",
        "                Example: 'computer'\n",
        "            num_results (int): Number of similar words to return\n",
        "                Example: 5\n",
        "            similarity_func (str): Similarity function to use ('cosine' or 'euclidean')\n",
        "\n",
        "        Returns:\n",
        "            list: List of tuples (word, similarity_score) for top num_results matches\n",
        "                Example: [('laptop', 0.89), ('pc', 0.87), ('desktop', 0.85), ...]\n",
        "\n",
        "        Note:\n",
        "            - Returns empty list if input word not in vocabulary\n",
        "            - Excludes the input word from results\n",
        "            - Implementation requires iterating through entire vocabulary using:\n",
        "              for word in self.word_vectors.index_to_key\n",
        "              This exhaustive search is needed to find the most similar words\n",
        "              by comparing the target word's vector against all known words\n",
        "        \"\"\"\n",
        "        # BEGIN CODE : word_embedding_ops.find_similar_words\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "# ==== END EVALUATION PORTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6obuQUDxanu4"
      },
      "outputs": [],
      "source": [
        "word_embedding_ops = WordEmbeddingOps(word2vec_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaaEvxluanu5"
      },
      "source": [
        " ## The Classic King-Man-Woman-Queen Analogy\n",
        "\n",
        " This famous analogy demonstrates how Word2Vec captures gender relationships:\n",
        " - king is to man as queen is to woman\n",
        " - Mathematically: king - man + woman ≈ queen\n",
        "\n",
        " This relationship emerged naturally during training, showing how embeddings learn semantic patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5DPWzBJanu6"
      },
      "outputs": [],
      "source": [
        "def demonstrate_king_man_queen_analogy():\n",
        "    \"\"\"\n",
        "    Demonstrate the famous king:man::queen:woman analogy.\n",
        "\n",
        "    Note:\n",
        "        - Shows results using both cosine and euclidean similarity\n",
        "        - Prints intermediate vectors and calculations\n",
        "        - Useful for understanding how word analogies work\n",
        "    \"\"\"\n",
        "    print(\"Testing famous analogy: king:man::queen:?\")\n",
        "\n",
        "    # Try with cosine similarity\n",
        "    results_cos = word_embedding_ops.find_analogies(\n",
        "        \"king\", \"man\", \"queen\", similarity_func=\"cosine\")\n",
        "    print(\"\\nUsing cosine similarity:\")\n",
        "    for word, score in results_cos:\n",
        "        print(f\"  {word}: {score:.3f}\")\n",
        "\n",
        "    # Try with euclidean similarity\n",
        "    results_euc = word_embedding_ops.find_analogies(\n",
        "        \"king\", \"man\", \"queen\", similarity_func=\"euclidean\")\n",
        "    print(\"\\nUsing euclidean similarity:\")\n",
        "    for word, score in results_euc:\n",
        "        print(f\"  {word}: {score:.3f}\")\n",
        "\n",
        "demonstrate_king_man_queen_analogy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grD4Wpy6anu7"
      },
      "source": [
        " ## Examining Gender Bias in Word Embeddings\n",
        "\n",
        " Word embeddings can reflect and amplify societal biases present in training data. Bolukbasi et al. (2016) in \"Man is to Computer Programmer as Woman is to Homemaker?\" demonstrated systematic gender biases in word embeddings.\n",
        "\n",
        " Common problematic analogies:\n",
        " - man:doctor :: woman:nurse\n",
        " - father:businessman :: mother:housewife\n",
        "\n",
        " These biases can propagate through NLP systems, affecting downstream applications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euItGWYjanu7"
      },
      "outputs": [],
      "source": [
        "def demonstrate_gender_bias():\n",
        "    \"\"\"\n",
        "    Demonstrate the famous man:doctor::woman:nurse analogy.\n",
        "\n",
        "    Note:\n",
        "        - Shows results using both cosine and euclidean similarity\n",
        "        - Prints intermediate vectors and calculations\n",
        "        - Useful for understanding how word analogies work\n",
        "    \"\"\"\n",
        "\n",
        "    examples = [\n",
        "        (\"man\", \"doctor\", \"woman\"),\n",
        "        (\"father\", \"doctor\", \"mother\"),\n",
        "    ]\n",
        "\n",
        "    for word1, word2, word3 in examples:\n",
        "        print(f\"\\nTesting: {word1}:{word2}::{word3}:?\")\n",
        "\n",
        "        # Try with cosine similarity\n",
        "        results_cos = word_embedding_ops.find_analogies(\n",
        "            word1, word2, word3, similarity_func=\"cosine\")\n",
        "        print(\"\\nUsing cosine similarity:\")\n",
        "        for word, score in results_cos:\n",
        "            print(f\"  {word}: {score:.3f}\")\n",
        "\n",
        "        # Try with euclidean similarity\n",
        "        results_euc = word_embedding_ops.find_analogies(\n",
        "            word1, word2, word3, similarity_func=\"euclidean\")\n",
        "        print(\"\\nUsing euclidean similarity:\")\n",
        "        for word, score in results_euc:\n",
        "            print(f\"  {word}: {score:.3f}\")\n",
        "\n",
        "demonstrate_gender_bias()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0_pC3pSanu9"
      },
      "source": [
        " ## Word Similarity and Semantic Clustering\n",
        "\n",
        " Beyond analogies, word embeddings cluster semantically similar words. The example below shows example of similar word finding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VXGFSvHanu9"
      },
      "outputs": [],
      "source": [
        "def demonstrate_similar_words():\n",
        "    \"\"\"\n",
        "    Demonstrate finding similar words for multiple example words.\n",
        "\n",
        "    Note:\n",
        "        - Tests similarity for words: cat, india, book, computer, phone\n",
        "        - Shows results using both cosine and euclidean similarity\n",
        "        - Prints top 5 similar words for each test word\n",
        "    \"\"\"\n",
        "    test_words = ['india', 'book']\n",
        "\n",
        "    for word in test_words:\n",
        "        print(f\"\\nFinding similar words for: {word}\")\n",
        "\n",
        "        # Try with cosine similarity\n",
        "        cos_similar = word_embedding_ops.find_similar_words(\n",
        "            word, similarity_func='cosine')\n",
        "        print(\"Using cosine similarity:\")\n",
        "        for similar_word, score in cos_similar:\n",
        "            print(f\"  {similar_word}: {score:.3f}\")\n",
        "\n",
        "        # Try with euclidean similarity\n",
        "        euc_similar = word_embedding_ops.find_similar_words(\n",
        "            word, similarity_func='euclidean')\n",
        "        print(\"\\nUsing euclidean similarity:\")\n",
        "        for similar_word, score in euc_similar:\n",
        "            print(f\"  {similar_word}: {score:.3f}\")\n",
        "\n",
        "demonstrate_similar_words()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VVTTjgwanu-"
      },
      "source": [
        "# Discriminative Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgSqqR-Yanu_"
      },
      "source": [
        " ## Bag of Words (BoW) Text Classifier\n",
        "\n",
        " This class implements text classification using the Bag of Words approach:\n",
        " 1. Convert text to word count vectors\n",
        " 2. Train logistic regression on these vectors\n",
        " 3. Make predictions on new text\n",
        "\n",
        " Features:\n",
        " - Text preprocessing (lowercase, remove punctuation, stop words)\n",
        " - Vocabulary creation from training data\n",
        " - Word count vectorization\n",
        " - Classification using logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXOAqL8PanvA"
      },
      "outputs": [],
      "source": [
        "# ==== BEGIN EVALUATION PORTION\n",
        "\n",
        "class BagOfWordsClassifier:\n",
        "    def __init__(self, min_freq=1):\n",
        "        \"\"\"\n",
        "        Initialize the Bag of Words classifier.\n",
        "\n",
        "        Args:\n",
        "            min_freq (int): Minimum frequency threshold for a word to be included in vocabulary.\n",
        "                           Words appearing less than min_freq times will be treated as UNK token.\n",
        "                           Default: 1 (include all words)\n",
        "\n",
        "        Attributes:\n",
        "            vocabulary (dict): Word to index mapping, including special UNK token\n",
        "                Example: {'<UNK>': 0, 'good': 1, 'movie': 2}\n",
        "            classifier: Trained logistic regression model\n",
        "            min_freq (int): Minimum frequency threshold for vocabulary inclusion\n",
        "                Example: If min_freq=2, words must appear at least twice to be included\n",
        "\n",
        "        Note:\n",
        "            - Words appearing less than min_freq times will be mapped to <UNK>\n",
        "            - <UNK> token is automatically added to vocabulary as first token (index 0)\n",
        "            - Logistic regression is used as the underlying classifier\n",
        "        \"\"\"\n",
        "        self.vocabulary = None\n",
        "        self.classifier = LogisticRegression(random_state=42)\n",
        "        self.min_freq = min_freq\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Preprocess text by converting to lowercase, removing punctuation,\n",
        "        and filtering stop words.\n",
        "\n",
        "        Args:\n",
        "            text (str): Raw input text\n",
        "                Example: \"This movie was really good!\"\n",
        "\n",
        "        Returns:\n",
        "            list: Cleaned and tokenized words\n",
        "                Example: ['movie', 'really', 'good']\n",
        "\n",
        "        Note:\n",
        "            - Converts all text to lowercase\n",
        "            - Removes punctuation and special characters\n",
        "            - Splits text into individual tokens\n",
        "            - Removes common English stop words\n",
        "            - Stop words are removed using NLTK's English stop words list\n",
        "        \"\"\"\n",
        "        from nltk.corpus import stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        text = text.lower()\n",
        "        tokens = re.findall(r'\\w+', text)\n",
        "        return [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    def create_vocabulary(self, texts):\n",
        "        \"\"\"\n",
        "        Create vocabulary from training texts by mapping each unique word to an index,\n",
        "        considering minimum frequency threshold and adding UNK token.\n",
        "\n",
        "        Args:\n",
        "            texts (list): List of text documents\n",
        "                Example: [\n",
        "                    \"good movie good\",\n",
        "                    \"bad movie\",\n",
        "                    \"great action movie\"\n",
        "                ]\n",
        "\n",
        "        Returns:\n",
        "            dict: Word to index mapping, including UNK token\n",
        "                Example (with min_freq=2): {\n",
        "                    '<UNK>': 0,    # Special token for rare/unseen words\n",
        "                    'movie': 1,    # Frequency=3, included in vocab\n",
        "                    'good': 2,     # Frequency=2, included in vocab\n",
        "                    # 'bad', 'great', 'action' not included (frequency=1 < min_freq=2)\n",
        "                }\n",
        "\n",
        "        Note:\n",
        "            - Always includes <UNK> token at index 0\n",
        "            - Only includes words that appear >= min_freq times\n",
        "            - Word frequency is counted across all documents\n",
        "            - Uses preprocess_text function for preprocessing\n",
        "            - Words below frequency threshold will be mapped to UNK during feature extraction\n",
        "        \"\"\"\n",
        "        # BEGIN CODE : bow.create_vocabulary\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def text_to_bow(self, texts):\n",
        "        \"\"\"\n",
        "        Convert texts to bag-of-words feature vectors using the vocabulary,\n",
        "        where each element represents the count of word occurrences (not binary presence/absence).\n",
        "        Words not in vocabulary are mapped to UNK token.\n",
        "\n",
        "        Args:\n",
        "            texts (list): List of text documents\n",
        "                Example: [\"good movie good watch\", \"bad movie skip\"]\n",
        "\n",
        "        Returns:\n",
        "            np.array: Document-term matrix with UNK handling\n",
        "\n",
        "            Example: For vocabulary {'<UNK>':0, 'movie':1, 'good':2} with min_freq=2:\n",
        "            array([[1, 1, 2],    # First doc: 1 UNK ('watch'), 1 'movie', 2 'good'\n",
        "                  [2, 1, 0]])    # Second doc: 2 UNKs ('bad','skip'), 1 'movie', 0 'good'\n",
        "\n",
        "        Note:\n",
        "            - First column represents count of UNK tokens\n",
        "            - Words not in vocabulary are mapped to UNK token (index 0)\n",
        "            - Uses preprocess_text function for preprocessing\n",
        "            - Shape of output: (n_documents, len(vocabulary))\n",
        "        \"\"\"\n",
        "        # BEGIN CODE : bow.text_to_bow\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def fit(self, X_text, y):\n",
        "        \"\"\"\n",
        "        Train the classifier on text documents.\n",
        "\n",
        "        Args:\n",
        "            X_text (list): List of text documents\n",
        "                Example: [\"good movie\", \"bad film\", \"great movie\"]\n",
        "            y (list): Class labels\n",
        "                Example: [1, 0, 1]  # 1=positive, 0=negative\n",
        "\n",
        "        Note:\n",
        "            - Creates vocabulary from training texts using min_freq threshold\n",
        "            - Converts texts to BoW features with UNK handling\n",
        "            - Trains logistic regression classifier on features\n",
        "        \"\"\"\n",
        "        # Create vocabulary from training texts\n",
        "        self.vocabulary = self.create_vocabulary(X_text)\n",
        "\n",
        "        # Convert texts to BoW features\n",
        "        X_bow = self.text_to_bow(X_text)\n",
        "\n",
        "        # Train classifier\n",
        "        self.classifier.fit(X_bow, y)\n",
        "\n",
        "    def predict(self, X_text):\n",
        "        \"\"\"\n",
        "        Predict classes for new documents.\n",
        "\n",
        "        Args:\n",
        "            X_text (list): List of text documents\n",
        "                Example: [\"amazing film\", \"terrible movie\"]\n",
        "\n",
        "        Returns:\n",
        "            list: Predicted class labels\n",
        "                Example: [1, 0]  # 1=positive, 0=negative\n",
        "\n",
        "        Note:\n",
        "            - Unknown words in test documents are mapped to UNK token\n",
        "            - Uses the same preprocessing as training\n",
        "        \"\"\"\n",
        "        # Convert texts to BoW features\n",
        "        X_bow = self.text_to_bow(X_text)\n",
        "\n",
        "        # Make predictions\n",
        "        return self.classifier.predict(X_bow)\n",
        "\n",
        "    def get_class_probabilities(self, X_text):\n",
        "        \"\"\"\n",
        "        Calculate prediction confidence scores for each class.\n",
        "\n",
        "        Args:\n",
        "            X_text (list): List of text documents\n",
        "                Example: [\"amazing film\", \"terrible movie\"]\n",
        "\n",
        "        Returns:\n",
        "            np.array: Confidence scores for each class (values 0-1)\n",
        "                Example: array([[0.1, 0.9],  # 90% confidence for positive class\n",
        "                              [0.8, 0.2]])   # 20% confidence for positive class\n",
        "\n",
        "        Note:\n",
        "            - Returns probability distribution over classes\n",
        "            - Each row sums to 1.0\n",
        "            - For binary classification:\n",
        "                - First column: confidence for negative class (0)\n",
        "                - Second column: confidence for positive class (1)\n",
        "            - Unknown words are handled via UNK token\n",
        "        \"\"\"\n",
        "        X_bow = self.text_to_bow(X_text)\n",
        "        return self.classifier.predict_proba(X_bow)\n",
        "\n",
        "# ==== END EVALUATION PORTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRF0qW-UanvC"
      },
      "source": [
        " ## Word2Vec Text Classifier\n",
        "\n",
        " This class implements text classification using Word2Vec embeddings:\n",
        " 1. Load pre-trained word vectors\n",
        " 2. Represent each document as average of its word vectors\n",
        " 3. Train logistic regression on these dense vectors\n",
        "\n",
        " Features:\n",
        " - Text preprocessing\n",
        " - Document representation using word embeddings\n",
        " - Classification using logistic regression\n",
        " - Support for different similarity metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ne5oatmanvD"
      },
      "outputs": [],
      "source": [
        "# ==== BEGIN EVALUATION PORTION\n",
        "\n",
        "class Word2VecClassifier:\n",
        "    def __init__(self, word2vec_path):\n",
        "        \"\"\"\n",
        "        Initialize Word2Vec classifier.\n",
        "\n",
        "        Args:\n",
        "            word2vec_path (str): Path to pre-trained word2vec model\n",
        "                Example: 'path/to/GoogleNews-vectors-negative300.bin'\n",
        "\n",
        "        Attributes:\n",
        "            word_vectors: Loaded word vectors\n",
        "            classifier: Trained logistic regression model\n",
        "        \"\"\"\n",
        "        self.word_vectors = KeyedVectors.load_word2vec_format(\n",
        "            word2vec_path, binary=True)\n",
        "        self.classifier = LogisticRegression(random_state=42)\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Preprocess text by converting to lowercase, removing punctuation,\n",
        "        and filtering stop words.\n",
        "\n",
        "        Args:\n",
        "            text (str): Raw input text\n",
        "                Example: \"This movie was really good!\"\n",
        "\n",
        "        Returns:\n",
        "            list: Cleaned and tokenized words\n",
        "                Example: ['movie', 'really', 'good']\n",
        "        \"\"\"\n",
        "        from nltk.corpus import stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        text = text.lower()\n",
        "        tokens = re.findall(r'\\w+', text)\n",
        "        return [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    def text_to_vec(self, texts):\n",
        "        \"\"\"\n",
        "        Convert texts to document vectors by averaging word embeddings.\n",
        "\n",
        "        Args:\n",
        "            texts (list): List of text documents\n",
        "                Example: [\"good movie\", \"bad film\"]\n",
        "\n",
        "        Returns:\n",
        "            np.array: Document vectors where each vector is average of its word vectors\n",
        "                Example shape: array([[0.2, 0.3, ..., -0.1],  # 300D vector for doc1\n",
        "                                    [0.1, 0.4, ..., -0.2]])   # 300D vector for doc2\n",
        "\n",
        "        Process:\n",
        "            1. For each document:\n",
        "                a. Split into words and preprocess\n",
        "                b. Look up word2vec vector for each word\n",
        "                c. Calculate mean of all word vectors in document\n",
        "                   - e.g., if doc has words [w1, w2, w3]:\n",
        "                     doc_vector = (vector(w1) + vector(w2) + vector(w3)) / 3\n",
        "                d. If no words found in vocabulary, vector remains zero\n",
        "\n",
        "        Note:\n",
        "            - Implementation hint: Vector size can be obtained using self.word_vectors.vector_size\n",
        "            - Each document vector has same dimensions as word vectors (e.g., 300)\n",
        "            - Words not in word2vec vocabulary are skipped\n",
        "            - Document vector is average of all found word vectors\n",
        "            - Documents with no known words get zero vectors\n",
        "            - You must use the preprocess_text function for pre-processing\n",
        "        \"\"\"\n",
        "\n",
        "        # BEGIN CODE : word2vec.text_to_vec\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # END CODE\n",
        "\n",
        "    def fit(self, X_text, y):\n",
        "        \"\"\"\n",
        "        Train classifier on text documents.\n",
        "\n",
        "        Args:\n",
        "            X_text (list): List of text documents\n",
        "                Example: [\"good movie\", \"bad film\", \"great movie\"]\n",
        "            y (list): Class labels\n",
        "                Example: [1, 0, 1]  # 1=positive, 0=negative\n",
        "        \"\"\"\n",
        "        # Convert texts to document vectors\n",
        "        X_vecs = self.text_to_vec(X_text)\n",
        "\n",
        "        # Train classifier\n",
        "        self.classifier.fit(X_vecs, y)\n",
        "\n",
        "    def predict(self, X_text):\n",
        "        \"\"\"\n",
        "        Predict classes for new documents.\n",
        "\n",
        "        Args:\n",
        "            X_text (list): List of text documents\n",
        "                Example: [\"amazing film\", \"terrible movie\"]\n",
        "\n",
        "        Returns:\n",
        "            list: Predicted class labels\n",
        "                Example: [1, 0]  # 1=positive, 0=negative\n",
        "        \"\"\"\n",
        "        X_vecs = self.text_to_vec(X_text)\n",
        "        return self.classifier.predict(X_vecs)\n",
        "\n",
        "    def get_class_probabilities(self, X_text):\n",
        "        \"\"\"\n",
        "        Calculate prediction confidence scores for each class.\n",
        "\n",
        "        Args:\n",
        "            X_text (list): List of text documents\n",
        "                Example: [\"amazing film\", \"terrible movie\"]\n",
        "\n",
        "        Returns:\n",
        "            np.array: Confidence scores for each class (values 0-1)\n",
        "                Example: array([[0.1, 0.9],  # 90% confidence for positive class\n",
        "                              [0.8, 0.2]])   # 20% confidence for positive class\n",
        "\n",
        "        Note:\n",
        "            - Returns probability distribution over classes\n",
        "            - Each row sums to 1.0\n",
        "            - For binary classification:\n",
        "                - First column: confidence for negative class (0)\n",
        "                - Second column: confidence for positive class (1)\n",
        "        \"\"\"\n",
        "        X_vecs = self.text_to_vec(X_text)\n",
        "        return self.classifier.predict_proba(X_vecs)\n",
        "\n",
        "# ==== END EVALUATION PORTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPVuyc5VanvG"
      },
      "source": [
        " ## Training and Evaluation\n",
        "\n",
        " Now we'll train both classifiers and evaluate their performance on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-ZCHZV1anvH"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate BoW classifier\n",
        "print(\"Training Bag of Words classifier...\")\n",
        "bow_clf = BagOfWordsClassifier()\n",
        "bow_clf.fit(X_train, y_train)\n",
        "\n",
        "bow_predictions = bow_clf.predict(X_val)\n",
        "bow_accuracy = evaluate_classifier(y_val, bow_predictions)\n",
        "\n",
        "# Above 80% validation accuracy is good!\n",
        "print(f\"BoW Validation Accuracy: {bow_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g3MqkcManvH"
      },
      "outputs": [],
      "source": [
        "print(\"\\nTraining Word2Vec classifier...\")\n",
        "word2vec_path = download_word2vec_model()\n",
        "w2v_clf = Word2VecClassifier(word2vec_path)\n",
        "w2v_clf.fit(X_train, y_train)\n",
        "\n",
        "w2v_predictions = w2v_clf.predict(X_val)\n",
        "w2v_accuracy = evaluate_classifier(y_val, w2v_predictions)\n",
        "\n",
        "# Above 80% validation accuracy is good!\n",
        "print(f\"Word2Vec Validation Accuracy: {w2v_accuracy:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ds-207-v1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}